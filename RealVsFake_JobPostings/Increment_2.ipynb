{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Increment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WAkEPIPdTEa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "6082e08e-2846-4108-ff98-1d48c788a02a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from string import punctuation\n",
        "import string\n",
        "from nltk import pos_tag\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords') \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/fake_job_postings.csv\")\n",
        "df.fillna(\" \", inplace=True)\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop.update(punctuation)\n",
        "\n",
        "df['final'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + ' ' + df['company_profile'] + ' ' + df[\n",
        "    'description'] + ' ' + df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + ' ' + df[\n",
        "                 'required_education'] + ' ' + df['industry'] + ' ' + df['function']\n",
        "print(df.final)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    final_text = []\n",
        "    for t in df.final[:50]:\n",
        "        word = lemmatizer.lemmatize(t, pos=\"n\")\n",
        "        final_text.append(words.lower())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "\n",
        "df.final = df.final.apply(lemmatize_words)\n",
        "\n",
        "\n",
        "\n",
        "train_text, test_text, train_category, test_category = train_test_split(df.final, df.fraudulent, test_size=0.2,random_state=0)\n",
        "                                                                        \n",
        "\n",
        "tfidf_Vect = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_Vect.fit_transform(df['text'].apply(lambda x: np.str_(x)))\n",
        "X_test_tfidf = tfidf_Vect.transform(df['fraudulent'].apply(lambda x: np.str_(x)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "lc = svm.SVC(kernel=\"linear\")\n",
        "lc.fit(X_train_tfidf,df.fraudulent )\n",
        "y_pred = lc.predict(X_test_tfidf)\n",
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(df['fraudulent'], y_pred))\n",
        "print(\"classification_report\\n\",metrics.classification_report(df['fraudulent'],y_pred))\n",
        "print(\"confusion matrix\\n\",metrics.confusion_matrix(df['fraudulent'],y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "0        Marketing Intern US, NY, New York Marketing We...\n",
            "1        Customer Service - Cloud Video Production NZ, ...\n",
            "2        Commissioning Machinery Assistant (CMA) US, IA...\n",
            "3        Account Executive - Washington DC US, DC, Wash...\n",
            "4        Bill Review Manager US, FL, Fort Worth   SpotS...\n",
            "                               ...                        \n",
            "17875    Account Director - Distribution  CA, ON, Toron...\n",
            "17876    Payroll Accountant US, PA, Philadelphia Accoun...\n",
            "17877    Project Cost Control Staff Engineer - Cost Con...\n",
            "17878    Graphic Designer NG, LA, Lagos     Nemsia Stud...\n",
            "17879    Web Application Developers NZ, N, Wellington E...\n",
            "Name: text, Length: 17880, dtype: object\n",
            "Accuracy: 0.9515659955257271\n",
            "classification_report\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98     17014\n",
            "           1       0.00      0.00      0.00       866\n",
            "\n",
            "    accuracy                           0.95     17880\n",
            "   macro avg       0.48      0.50      0.49     17880\n",
            "weighted avg       0.91      0.95      0.93     17880\n",
            "\n",
            "confusion matrix\n",
            " [[17014     0]\n",
            " [  866     0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_clGGXZO3fI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c30846cc-751d-4bee-8ed2-2e9ce3445953"
      },
      "source": [
        "\n",
        "tfidf_Vect = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_Vect.fit_transform(df['text'].apply(lambda x: np.str_(x)))\n",
        "clf = KNeighborsClassifier()\n",
        "clf.fit(X_train_tfidf, df.fraudulent)\n",
        "\n",
        "X_test_tfidf = tfidf_Vect.transform(df['fraudulent'].apply(lambda x: np.str_(x)))\n",
        "\n",
        "predicted = clf.predict(X_test_tfidf)\n",
        "\n",
        "scoreAccuracy = metrics.accuracy_score(df['fraudulent'], predicted)\n",
        "print(scoreAccuracy)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9515659955257271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBEdxJOzut4b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "72b72dc6-47a3-44f9-8915-ff2a0787b92f"
      },
      "source": [
        "                            \n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "my_model = Sequential() # create model\n",
        "my_model.add(Dense(64, input_dim=104998, activation='relu')) # hidden layer\n",
        "my_model.add(Dense(32, activation='relu'))\n",
        "my_model.add(Dense(1, activation='sigmoid')) # output layer\n",
        "my_model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['acc'])\n",
        "my_model_fit = my_model.fit(X_train_tfidf, df['fraudulent'], epochs=10, initial_epoch=0)\n",
        "print(my_model.summary())\n",
        "print(my_model.evaluate(X_test_tfidf,df['fraudulent']))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "17880/17880 [==============================] - 40s 2ms/step - loss: 0.1331 - acc: 0.9620\n",
            "Epoch 2/10\n",
            "17880/17880 [==============================] - 39s 2ms/step - loss: 0.0436 - acc: 0.9856\n",
            "Epoch 3/10\n",
            "17880/17880 [==============================] - 40s 2ms/step - loss: 0.0244 - acc: 0.9932\n",
            "Epoch 4/10\n",
            "17880/17880 [==============================] - 41s 2ms/step - loss: 0.0149 - acc: 0.9957\n",
            "Epoch 5/10\n",
            "17880/17880 [==============================] - 41s 2ms/step - loss: 0.0089 - acc: 0.9977\n",
            "Epoch 6/10\n",
            "17880/17880 [==============================] - 41s 2ms/step - loss: 0.0055 - acc: 0.9985\n",
            "Epoch 7/10\n",
            "17880/17880 [==============================] - 41s 2ms/step - loss: 0.0037 - acc: 0.9989\n",
            "Epoch 8/10\n",
            "17880/17880 [==============================] - 41s 2ms/step - loss: 0.0025 - acc: 0.9994\n",
            "Epoch 9/10\n",
            "17880/17880 [==============================] - 41s 2ms/step - loss: 0.0018 - acc: 0.9996\n",
            "Epoch 10/10\n",
            "17880/17880 [==============================] - 41s 2ms/step - loss: 0.0014 - acc: 0.9996\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 64)                6719936   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 6,722,049\n",
            "Trainable params: 6,722,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "17880/17880 [==============================] - 11s 606us/step\n",
            "[3.8328232998749288, 0.04843400418758392]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}